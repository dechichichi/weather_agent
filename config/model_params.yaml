transformer:
  #模型中每个位置的输入和输出向量的维度
  d_model: 512
  # 多头自注意力机制中头的数量
  nhead: 8
  # 堆叠的编码器或解码器层数为6层
  num_layers: 6
  # 在训练过程中，有10%的概率随机丢弃一些神经元，以防止模型过拟合。
  dropout: 0.1